
# Data Analyst Engineer Assignment - Submission Readme

## Project Structure

This repository contains the submission for the Data Analyst Engineer assignment, organized into three main folders:

1. Folder 1 - SQL Queries
2. Folder 2 - Data Pipeline
3. Folder 3 - Cohort Analysis

# Folder 1 - SQL Queries
Contains three SQL files corresponding to the SQL proficiency test queries.

1. query1.sql: Lists the total number of orders for each product, sorted by the number of orders.

2. query2.sql: Finds the total revenue generated by each user.

3. query3.sql: Displays users who have not placed any orders.

# Folder 2 - Data Pipeline

Includes three Python files and three CSV files for data pipeline construction.

1. main.py: The main script to run the data pipeline.
2. database.py: This contains all the database functions
3. csv.py: This contains functions to get data from csv and push them into PostgreSQL database
4. database_operations.log: Logs for the data pipeline operations.
5. requirements.txt: Lists Python dependencies.
6. users.csv, orders.csv, products.csv: CSV files corresponding to Users, Orders, and Products tables.

## Running the Data Pipeline:

Step 1: Make sure to fill .env properly

Step 2 : Create a postgreSQL database corresponding to the information you have entered in .env

Step 3 : Install all dependencies inside requirements.txt
```
pip install -r requirements.txt
```

Step 4: Execute main.py to run the data pipeline. Logs will be saved in database_operations.log.

```
python3 main.py
```

# Folder 3 - Cohort Analysis

Contains an SQL file and a CSV file for cohort analysis.

1. cohort_analysis.sql: SQL commands for cohort analysis.
2. cohort_analysis_output.csv: CSV file containing the output of the cohort analysis.